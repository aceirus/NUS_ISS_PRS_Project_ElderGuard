{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "curSEImjHv5n"
   },
   "source": [
    "# SMS Spam Detection Using Machine Learning\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## Practice Module: Pattern Recognition Systems (PRS)\n",
    "\n",
    "## Group: 18\n",
    "\n",
    "## Members:\n",
    "\n",
    "Lim Jun Ming, A0231523U\n",
    "\n",
    "Mediana, A0231458E\n",
    "\n",
    "Yeong Wee Ping, A0231533R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cKLQgQNqHv3E"
   },
   "source": [
    "# Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WxYEblO9IAZ0"
   },
   "source": [
    "## 0. File Path & Library Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 553,
     "status": "ok",
     "timestamp": 1631883048634,
     "user": {
      "displayName": "JM Lim",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12752115953509725575"
     },
     "user_tz": -480
    },
    "id": "xVMjtjxeIELI",
    "outputId": "48edf3e5-cc79-4e85-8972-e6a60adadd4a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "Versions of key libraries\n",
      "-------------------------\n",
      "pandas:   1.1.5\n",
      "numpy:    1.19.5\n",
      "nltk:     3.2.5\n"
     ]
    }
   ],
   "source": [
    "# Load All Necessary Packages\n",
    "\n",
    "import os\n",
    "from google.colab import drive\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "seed = 18\n",
    "\n",
    "print('Versions of key libraries')\n",
    "print('-------------------------')\n",
    "print('pandas:  ', pd.__version__)\n",
    "print('numpy:   ', np.__version__)\n",
    "print('nltk:    ', nltk.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1631883048635,
     "user": {
      "displayName": "JM Lim",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12752115953509725575"
     },
     "user_tz": -480
    },
    "id": "E4UpGlniIFwK",
    "outputId": "ef455cbb-d3a6-47b0-f5b0-a4444b07bb1b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
      "Working Directory: \n",
      "/content/gdrive/My Drive/iss/prs_pm/training\n"
     ]
    }
   ],
   "source": [
    "# Mounting to Google Drive\n",
    "drive.mount('/content/gdrive')\n",
    "\n",
    "# Change Working Directory\n",
    "os.chdir('/content/gdrive/My Drive/iss/prs_pm/training')\n",
    "\n",
    "print('Working Directory: ')\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6yEB1N4xIKj2"
   },
   "source": [
    "## 1. Text Preprocessing Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rGMGIdQbQsUa"
   },
   "source": [
    "1. Lower Casing - Using simple string lower case for each tokens in each sms.\n",
    "2. Remove numbers and punctuations\n",
    "3. Tokenization - Using simple tokenisation method by splitting each sms into a list of words without punctuations.\n",
    "4. Stopwords Removal - Using nltk english stopword corpus and custom stopword dictionary to remove all stopwords from sms.\n",
    "5. Lemmatization\n",
    "\n",
    "Simple tokenisation and stopword removal are used as often in sms context, words used are not in their proper form. Hence, the original words from the sms is remained in the processed text with punctuation removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IZwnAwHYGYT_"
   },
   "outputs": [],
   "source": [
    "# Text processing function\n",
    "stopwordsdic = stopwords.words('english') # stopwords dictionary\n",
    "customstopwords = ['u', 'ur']\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def text_process(sms):\n",
    "    text = sms.lower() # Lower casing \n",
    "    text = text.strip() # Strip off excess spacing in the end of message\n",
    "    text = re.sub(r'\\d+', '', text) # Remove all numbers\n",
    "    text = re.sub('[^A-Za-z0-9]+', ' ', text) # Remove punctuations\n",
    "    text = word_tokenize(text) # Tokenization\n",
    "    text = [word for word in text if word not in stopwordsdic] # Remove stopwords\n",
    "    text = [word for word in text if word not in customstopwords] # Remove custom stopwords\n",
    "    text = [lemmatizer.lemmatize(word) for word in text] # Lemmatization\n",
    "    return text # List of tokens\n",
    "\n",
    "def text_maker(tokens_list): # To join the processed words(tokens) back into a text\n",
    "    sent = ' '.join(tokens_list)\n",
    "    return sent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sos3tcerISnW"
   },
   "source": [
    "## 2. Load Structure Data and Apply Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0OabnK9-IYqQ"
   },
   "outputs": [],
   "source": [
    "# Load Data\n",
    "header = ['Label', 'Text']\n",
    "data = pd.read_csv('structured_data/smsdata.csv', encoding='UTF-8', names=header)\n",
    "\n",
    "# Apply Text Preprocessing\n",
    "data['Processed Token'] = data[\"Text\"].apply(lambda x: text_process(x))\n",
    "\n",
    "# Apply Text Make (To combined tokens back into a string for easy storage)\n",
    "data['Processed Text'] = data['Processed Token'].apply(lambda x: text_maker(x))\n",
    "\n",
    "# Preprocessed Dataset\n",
    "data_text = data[['Label', 'Processed Text']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1631883050256,
     "user": {
      "displayName": "JM Lim",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12752115953509725575"
     },
     "user_tz": -480
    },
    "id": "PuBY6v5pYx7N",
    "outputId": "ad15cd7c-b6ba-441e-9e37-6880eb78a422"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed dataset has 5837 rows of labelled data points\n",
      "-----------------------------------------------------------\n",
      "  Label                                     Processed Text\n",
      "0   ham                          dunno let go learn pilate\n",
      "1   ham                                    wat time finish\n",
      "2   ham     would really appreciate call need someone talk\n",
      "3   ham                                   still grand prix\n",
      "4   ham  mmm thats better got roast b better drink good...\n",
      "-----------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Show data heads\n",
    "\n",
    "row, clm = data_text.shape\n",
    "\n",
    "print('Preprocessed dataset has {:2d} rows of labelled data points'.format(row))\n",
    "print('-----------------------------------------------------------')\n",
    "print(data_text.head())\n",
    "print('-----------------------------------------------------------')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eLF-ZHhqIY8c"
   },
   "source": [
    "## 3. Save Preprocessed Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EgbZCi9XIZ0G"
   },
   "outputs": [],
   "source": [
    "# Save Preprocessed Dataset\n",
    "data_text.to_csv('structured_data/procdata.csv', index=False, header=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMuB0HsEEx55Lhhe6QDHIDp",
   "collapsed_sections": [],
   "name": "Text_Preprocessing.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
