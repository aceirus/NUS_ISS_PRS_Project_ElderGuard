{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iaKjvTeQRaki"
   },
   "source": [
    "# SMS Spam Detection Using Machine Learning\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## Practice Module: Pattern Recognition Systems (PRS)\n",
    "\n",
    "## Group: 18\n",
    "\n",
    "## Members:\n",
    "\n",
    "Lim Jun Ming, A0231523U\n",
    "\n",
    "Mediana, A0231458E\n",
    "\n",
    "Yeong Wee Ping, A0231533R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wq6i6jnlRahJ"
   },
   "source": [
    "# Feature Engineering & Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y76XArfRRlVy"
   },
   "source": [
    "## 0. File Path & Library Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4129,
     "status": "ok",
     "timestamp": 1633764744078,
     "user": {
      "displayName": "JM Lim",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12752115953509725575"
     },
     "user_tz": -480
    },
    "id": "KEYZIb1URXCh",
    "outputId": "6c13493a-d1eb-458b-c0b0-747f13c3de25"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "Versions of key libraries\n",
      "-------------------------\n",
      "pandas:   1.1.5\n",
      "numpy:    1.19.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/externals/six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
      "  \"(https://pypi.org/project/six/).\", FutureWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.neighbors.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neighbors. Anything that cannot be imported from sklearn.neighbors is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Load All Necessary Packages\n",
    "\n",
    "import os\n",
    "import time\n",
    "from google.colab import drive\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "import fnmatch\n",
    "import nltk\n",
    "\n",
    "from collections import defaultdict\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "import h5py\n",
    "import pickle\n",
    "\n",
    "seed = 18\n",
    "\n",
    "print('Versions of key libraries')\n",
    "print('-------------------------')\n",
    "print('pandas:  ', pd.__version__)\n",
    "print('numpy:   ', np.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23769,
     "status": "ok",
     "timestamp": 1633764767845,
     "user": {
      "displayName": "JM Lim",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12752115953509725575"
     },
     "user_tz": -480
    },
    "id": "C4On28HgRpDt",
    "outputId": "587c421f-00ec-493f-874b-ec37f4e468f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/gdrive\n",
      "Working Directory: \n",
      "/content/gdrive/My Drive/iss/prs_pm/training\n"
     ]
    }
   ],
   "source": [
    "# Mounting to Google Drive\n",
    "drive.mount('/content/gdrive')\n",
    "\n",
    "# Change Working Directory\n",
    "os.chdir('/content/gdrive/My Drive/iss/prs_pm/training')\n",
    "\n",
    "print('Working Directory: ')\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5nvsIelhR6w4"
   },
   "source": [
    "## 1. Load Data and Train-Validation-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1106,
     "status": "ok",
     "timestamp": 1633764768950,
     "user": {
      "displayName": "JM Lim",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12752115953509725575"
     },
     "user_tz": -480
    },
    "id": "T5Rp5pnERw4I",
    "outputId": "1cb071b7-5509-44fd-ac9e-79844a87129b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of data points in training dataset    : 4085\n",
      "Total number of data points in validation dataset  : 876\n",
      "Total number of data points in testing dataset     : 876\n"
     ]
    }
   ],
   "source": [
    "# Train Test Set Split\n",
    "header = ['Label', 'Text']\n",
    "rawdata = pd.read_csv('structured_data/smsdata.csv', encoding='UTF-8', names=header)\n",
    "data = pd.read_csv('structured_data/procdata.csv', encoding='UTF-8', names=header)\n",
    "\n",
    "data['Text'] = data['Text'].astype(str)\n",
    "data['Text_Token'] = data['Text'].apply(lambda x: x.split(' '))\n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "X_data = data[\"Text\"]\n",
    "y_data = le.fit_transform(data[\"Label\"])\n",
    "\n",
    "# Split Train-Validation-Test set 0.7-0.15-0.15 ratio\n",
    "feature_train, feature_valntest, y_train, y_valntest = train_test_split(X_data, y_data, test_size=0.3, stratify=y_data, random_state=seed) \n",
    "\n",
    "feature_val, feature_test, y_val, y_test = train_test_split(feature_valntest, y_valntest, test_size=0.5, stratify=y_valntest, random_state=seed)\n",
    "\n",
    "train_idx = feature_train.index\n",
    "val_idx = feature_val.index\n",
    "test_idx = feature_test.index\n",
    "\n",
    "X_train_token = data['Text_Token'][train_idx] \n",
    "X_val_token = data['Text_Token'][val_idx]\n",
    "X_test_token = data['Text_Token'][test_idx] \n",
    "\n",
    "X_train_sent = data['Text'][train_idx] \n",
    "X_val_sent = data['Text'][val_idx] \n",
    "X_test_sent = data['Text'][test_idx] \n",
    "\n",
    "y_train_sm = np.append(y_train, np.ones((y_train==0).sum()-(y_train==1).sum())).astype(np.int)\n",
    "\n",
    "print('Total number of data points in training dataset    : ' + str(feature_train.shape[0]))\n",
    "print('Total number of data points in validation dataset  : ' + str(feature_val.shape[0]))\n",
    "print('Total number of data points in testing dataset     : ' + str(feature_test.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 509,
     "status": "ok",
     "timestamp": 1633764769458,
     "user": {
      "displayName": "JM Lim",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12752115953509725575"
     },
     "user_tz": -480
    },
    "id": "4fnps0WJqbCd",
    "outputId": "52c8ed95-c2ac-4058-b8e9-5a4a899b3e87"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save Complete!\n"
     ]
    }
   ],
   "source": [
    "# Saving y label dataset into folder in pickle format\n",
    "y_pickle = [y_train, y_val, y_test]\n",
    "y_file = open('input_data/y_label.pickle', 'wb')\n",
    "pickle.dump(y_pickle, y_file)\n",
    "y_file.close()\n",
    "\n",
    "print('Save Complete!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1633764769459,
     "user": {
      "displayName": "JM Lim",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12752115953509725575"
     },
     "user_tz": -480
    },
    "id": "KU2J_gCApmtn"
   },
   "outputs": [],
   "source": [
    "# # Testing Load Speed\n",
    "\n",
    "# start = time.time()\n",
    "# pickle_load = pickle.load(open('input_data/y_label.pickle', 'rb'))\n",
    "# runspeed = round(time.time()-start,3)\n",
    "# print(runspeed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1633764769459,
     "user": {
      "displayName": "JM Lim",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12752115953509725575"
     },
     "user_tz": -480
    },
    "id": "RgGXIvCtUo4O"
   },
   "outputs": [],
   "source": [
    "# Create word dictionary sorted by frequency\n",
    "spam_words = \" \".join([word for msg in data['Text_Token'][data['Label']=='spam'] for word in msg]) # list of words\n",
    "\n",
    "# Create spam words dictionary\n",
    "spam_dic = defaultdict(int)\n",
    "for i in spam_words.split(\" \"):\n",
    "  spam_dic[i] += 1\n",
    "spam_dic_sorted = dict(sorted(spam_dic.items(), key=lambda item:item[1], reverse=True))\n",
    "spam_show = 20 # To show top 20 spam words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JNZ4UcdqTEOu"
   },
   "source": [
    "## Approach 1: Manual Feature Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1633764769460,
     "user": {
      "displayName": "JM Lim",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12752115953509725575"
     },
     "user_tz": -480
    },
    "id": "5YIeKzjeRyP8"
   },
   "outputs": [],
   "source": [
    "# Custom Functions for manual feature extractions\n",
    "# Count for the presence of mathematical symbols\n",
    "math_symbol = [\"+\", \"-\", \"/\", \"^\", \"<\", \">\"]\n",
    "def count_math_symbol(sms):\n",
    "    counter = 0\n",
    "    for i in math_symbol:\n",
    "      counter += sms.count(i)\n",
    "    return counter\n",
    "\n",
    "# Count for the presence of special symbols \n",
    "special_symbol = [\"!\", \"@\", \"$\", \"~\", \"#\", \"&\", \"*\"]\n",
    "def count_special_symbol(sms):\n",
    "    counter = 0\n",
    "    for i in special_symbol:\n",
    "      counter += sms.count(i)\n",
    "    return counter\n",
    "\n",
    "# Count for the presence of fully uppercased words\n",
    "def count_uppercase(sms):\n",
    "    counter = 0\n",
    "    text = sms.strip() # Strip off excess spacing in the end of message\n",
    "    text = re.sub(r'\\d+', '', text) # Remove all numbers\n",
    "    text = re.sub('[^A-Za-z0-9]+', ' ', text) # Remove punctuations\n",
    "    text = word_tokenize(text) # Tokenization\n",
    "    for i in text:\n",
    "      counter += i.isupper()\n",
    "    return counter\n",
    "\n",
    "# Count for the presence of the top 20 common words for spam messages\n",
    "# List of top 20 spam words\n",
    "top_spam_words = list(spam_dic_sorted.keys())[:20]\n",
    "# Function for counting spam words in processed text\n",
    "def count_spam_words(tokens):\n",
    "    counter = 0\n",
    "    for i in top_spam_words:\n",
    "      counter += tokens.count(i)\n",
    "    return counter\n",
    "\n",
    "# Indicator for presence of phone number (Taking consecutive numbers of length 5 or 8 and above as phone number)\n",
    "phoneno_format = [\"*xxxxx*\", \"*xxxx-xxxx*\", \"*xxxxxxxx*\", \"*+xx-xxxx-xxxx*\"]\n",
    "def check_phoneno(sms):\n",
    "    text = re.sub(r'\\d', 'x', sms)\n",
    "    indicator = 0\n",
    "    for i in phoneno_format:\n",
    "      if fnmatch.fnmatch(text, i):\n",
    "        indicator = 1\n",
    "        break\n",
    "    return indicator\n",
    "\n",
    "# Function to extract all manually designed features\n",
    "def mfd_extract(sms):\n",
    "    feature = []\n",
    "    feature.append(count_math_symbol(sms))\n",
    "    feature.append(count_special_symbol(sms))\n",
    "    feature.append(count_uppercase(sms))\n",
    "    # feature.append(count_spam_words(text_process(sms)))\n",
    "    feature.append(check_phoneno(sms))\n",
    "    # feature.append(len(text_process(sms)))\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1238,
     "status": "ok",
     "timestamp": 1633764770695,
     "user": {
      "displayName": "JM Lim",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12752115953509725575"
     },
     "user_tz": -480
    },
    "id": "uaYFm2GoRyKt",
    "outputId": "0b5898a7-ca06-4903-93dd-352db1f02ae0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Map for Train Dataset:\n",
      "\n",
      "      Math_Symbol  Special_Symbol  ...  Spam_Words  Num_WOrds\n",
      "1074          0.0             0.0  ...         0.0   0.097222\n",
      "1310          0.0             0.0  ...         0.0   0.055556\n",
      "5257          0.0             0.0  ...         0.2   0.041667\n",
      "597           0.0             0.0  ...         0.0   0.083333\n",
      "1530          0.0             0.0  ...         0.0   0.180556\n",
      "\n",
      "[5 rows x 6 columns]\n",
      "------------------------------------------------------------\n",
      "\n",
      "Feature Map for Validation Dataset:\n",
      "\n",
      "      Math_Symbol  Special_Symbol  ...  Spam_Words  Num_WOrds\n",
      "1711     0.000000        0.055556  ...         0.0   0.069444\n",
      "687      0.615385        0.000000  ...         0.1   0.236111\n",
      "1229     0.153846        0.018519  ...         0.0   0.027778\n",
      "3460     0.000000        0.000000  ...         0.0   0.138889\n",
      "617      0.000000        0.000000  ...         0.1   0.055556\n",
      "\n",
      "[5 rows x 6 columns]\n",
      "------------------------------------------------------------\n",
      "\n",
      "Feature Map for Test Dataset:\n",
      "\n",
      "      Math_Symbol  Special_Symbol  ...  Spam_Words  Num_WOrds\n",
      "3675     0.000000        0.000000  ...         0.0   0.013889\n",
      "1841     0.076923        0.037037  ...         0.3   0.263889\n",
      "519      0.000000        0.000000  ...         0.0   0.041667\n",
      "3314     0.000000        0.055556  ...         0.0   0.180556\n",
      "1603     0.000000        0.000000  ...         0.0   0.013889\n",
      "\n",
      "[5 rows x 6 columns]\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Creating the feature map dataframe (For manual feature design)\n",
    "feature_vector = rawdata['Text'].apply(lambda x: mfd_extract(x))\n",
    "feature_name = ['Math_Symbol', 'Special_Symbol', 'Uppercase_Words', 'Phone_Number']# On Raw Data\n",
    "data_mfd = pd.DataFrame(np.vstack(feature_vector), columns=feature_name) \n",
    "data_mfd['Spam_Words'] = data['Text_Token'].apply(lambda x: count_spam_words(x)) # Add feature: number of occurence of top 20 spam words\n",
    "data_mfd['Num_WOrds'] = data['Text_Token'].apply(lambda x: len(x)) # Add feature: length of preprocessed text\n",
    "X_train_mfd = data_mfd.loc[train_idx, :]\n",
    "X_val_mfd = data_mfd.loc[val_idx, :]\n",
    "X_test_mfd = data_mfd.loc[test_idx, :]\n",
    "\n",
    "\n",
    "# Normalization\n",
    "train_max = X_train_mfd.max() # Max Values\n",
    "train_min = X_train_mfd.min() # Min Values\n",
    "feature_name = X_train_mfd.columns\n",
    "for i in data_mfd.columns:\n",
    "  if i != 'Phone_Number': # Normalization exclude 'Phone_Number' feature\n",
    "    X_train_mfd[i] = X_train_mfd[i].apply(lambda x: (x - train_min[i])/(train_max[i]- train_min[i]))\n",
    "    X_val_mfd[i] = X_val_mfd[i].apply(lambda x: (x - train_min[i])/(train_max[i]- train_min[i]))\n",
    "    X_test_mfd[i] = X_test_mfd[i].apply(lambda x: (x - train_min[i])/(train_max[i]- train_min[i]))\n",
    "\n",
    "print('Feature Map for Train Dataset:\\n')\n",
    "print(X_train_mfd.head())\n",
    "print('------------------------------------------------------------\\n')\n",
    "print('Feature Map for Validation Dataset:\\n')\n",
    "print(X_val_mfd.head())\n",
    "print('------------------------------------------------------------\\n')\n",
    "print('Feature Map for Test Dataset:\\n')\n",
    "print(X_test_mfd.head())\n",
    "print('------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 308,
     "status": "ok",
     "timestamp": 1633764771001,
     "user": {
      "displayName": "JM Lim",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12752115953509725575"
     },
     "user_tz": -480
    },
    "id": "9x_70I7wFkQo"
   },
   "outputs": [],
   "source": [
    "# Saving Constants - Top 20 Spam Words\n",
    "np.savetxt('feature_extraction_constants/top20spamwords.txt', top_spam_words, delimiter=\" \", fmt=\"%s\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 305,
     "status": "ok",
     "timestamp": 1633764771304,
     "user": {
      "displayName": "JM Lim",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12752115953509725575"
     },
     "user_tz": -480
    },
    "id": "RUzDkgIjTKo5"
   },
   "outputs": [],
   "source": [
    "# Saving Constants - Max and Min Parameters Values for Normalization\n",
    "minmaxscale = pd.DataFrame(np.array([train_min, train_max]).transpose(), index=data_mfd.columns, columns=['Min', 'Max'])\n",
    "minmaxscale.to_csv('feature_extraction_constants/minmaxscale.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 388,
     "status": "ok",
     "timestamp": 1633764771691,
     "user": {
      "displayName": "JM Lim",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12752115953509725575"
     },
     "user_tz": -480
    },
    "id": "-bJd_8umzFoC",
    "outputId": "17df059a-b60c-4686-8241-e2300e1c8a63"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save Complete!\n"
     ]
    }
   ],
   "source": [
    "# Saving X_mfd dataset into folder in pickle format\n",
    "X_mfd_pickle = [X_train_mfd, X_val_mfd, X_test_mfd]\n",
    "X_mfd_file = open('input_data/X_mfd.pickle', 'wb')\n",
    "pickle.dump(X_mfd_pickle, X_mfd_file)\n",
    "X_mfd_file.close()\n",
    "\n",
    "print('Save Complete!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1633764771691,
     "user": {
      "displayName": "JM Lim",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12752115953509725575"
     },
     "user_tz": -480
    },
    "id": "sd_tuqe9z7e0"
   },
   "outputs": [],
   "source": [
    "# # Testing Load Speed\n",
    "\n",
    "# start = time.time()\n",
    "# pickle_load = pickle.load(open('input_data/X_mfd.pickle', 'rb'))\n",
    "# runspeed = round(time.time()-start,3)\n",
    "# print(runspeed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pJYxdPLEaT__"
   },
   "source": [
    "## Approach 2: Bag-Of-Words (BOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1633764771691,
     "user": {
      "displayName": "JM Lim",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12752115953509725575"
     },
     "user_tz": -480
    },
    "id": "dtCfg1fia6Md"
   },
   "outputs": [],
   "source": [
    "# Initialize Tensorflow Tokenizer\n",
    "tk = Tokenizer()\n",
    "\n",
    "# Fit Tokenizer\n",
    "X_train_tf = tk.fit_on_texts(X_train_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "szgpBRJLaa6n"
   },
   "source": [
    "### i. CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 284,
     "status": "ok",
     "timestamp": 1633764771973,
     "user": {
      "displayName": "JM Lim",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12752115953509725575"
     },
     "user_tz": -480
    },
    "id": "bCOBanvXaYKV",
    "outputId": "b483f15f-cf9a-42af-da4a-e115f1acbc2a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data feature space shape      : (4085, 6390)\n",
      "Validation data feature space shape : (876, 6390)\n",
      "Test data feature space shape       : (876, 6390)\n",
      "Size of Vocabulary                  : 6390\n"
     ]
    }
   ],
   "source": [
    "X_train_cv = tk.texts_to_matrix(X_train_sent, mode='count')\n",
    "X_val_cv = tk.texts_to_matrix(X_val_sent, mode='count')\n",
    "X_test_cv = tk.texts_to_matrix(X_test_sent, mode='count')\n",
    "\n",
    "print('Train data feature space shape      : ' + str(X_train_cv.shape))\n",
    "print('Validation data feature space shape : ' + str(X_val_cv.shape))\n",
    "print('Test data feature space shape       : ' + str(X_test_cv.shape))\n",
    "print('Size of Vocabulary                  : ' + str(X_train_cv.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3036,
     "status": "ok",
     "timestamp": 1633764775008,
     "user": {
      "displayName": "JM Lim",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12752115953509725575"
     },
     "user_tz": -480
    },
    "id": "Wa6dhNkk3XIB",
    "outputId": "259dd555-17f5-494e-deae-06ca78217b19"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save Complete!\n"
     ]
    }
   ],
   "source": [
    "# Saving X_cv dataset into folder in pickle format\n",
    "X_cv_pickle = [X_train_cv, X_val_cv, X_test_cv]\n",
    "X_cv_file = open('input_data/X_cv.pickle', 'wb')\n",
    "pickle.dump(X_cv_pickle, X_cv_file)\n",
    "X_cv_file.close()\n",
    "\n",
    "print('Save Complete!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1633764775008,
     "user": {
      "displayName": "JM Lim",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12752115953509725575"
     },
     "user_tz": -480
    },
    "id": "tuoPRABI3W_8"
   },
   "outputs": [],
   "source": [
    "# # Testing Load Speed\n",
    "\n",
    "# start = time.time()\n",
    "# pickle_load = pickle.load(open('input_data/X_cv.pickle', 'rb'))\n",
    "# runspeed = round(time.time()-start,3)\n",
    "# print(runspeed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JiiRV4uNezRU"
   },
   "source": [
    "### ii. Tf-Idf Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 490,
     "status": "ok",
     "timestamp": 1633764775496,
     "user": {
      "displayName": "JM Lim",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12752115953509725575"
     },
     "user_tz": -480
    },
    "id": "hT1HqiGbaYml",
    "outputId": "a58f367b-3c88-43ae-d868-38a7048bc732"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data feature space shape      : (4085, 6390)\n",
      "Validation data feature space shape : (876, 6390)\n",
      "Test data feature space shape       : (876, 6390)\n",
      "Size of Vocabulary                  : 6390\n"
     ]
    }
   ],
   "source": [
    "X_train_tfidf = tk.texts_to_matrix(X_train_sent, mode='tfidf')\n",
    "X_val_tfidf = tk.texts_to_matrix(X_val_sent, mode='tfidf')\n",
    "X_test_tfidf = tk.texts_to_matrix(X_test_sent, mode='tfidf')\n",
    "\n",
    "print('Train data feature space shape      : ' + str(X_train_tfidf.shape))\n",
    "print('Validation data feature space shape : ' + str(X_val_tfidf.shape))\n",
    "print('Test data feature space shape       : ' + str(X_test_tfidf.shape))\n",
    "print('Size of Vocabulary                  : ' + str(X_train_tfidf.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3053,
     "status": "ok",
     "timestamp": 1633764778544,
     "user": {
      "displayName": "JM Lim",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12752115953509725575"
     },
     "user_tz": -480
    },
    "id": "k9Tu2__c3xMK",
    "outputId": "975d868b-cebf-4761-a373-e6937b2ba70e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save Complete!\n"
     ]
    }
   ],
   "source": [
    "# Saving X_tfidf dataset into folder in pickle format\n",
    "X_tfidf_pickle = [X_train_tfidf, X_val_tfidf, X_test_tfidf]\n",
    "X_tfidf_file = open('input_data/X_tfidf.pickle', 'wb')\n",
    "pickle.dump(X_tfidf_pickle, X_tfidf_file)\n",
    "X_tfidf_file.close()\n",
    "\n",
    "print('Save Complete!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1633764778545,
     "user": {
      "displayName": "JM Lim",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12752115953509725575"
     },
     "user_tz": -480
    },
    "id": "vEUw2ipf3xFK"
   },
   "outputs": [],
   "source": [
    "# # Testing Load Speed\n",
    "\n",
    "# start = time.time()\n",
    "# pickle_load = pickle.load(open('input_data/X_tfidf.pickle', 'rb'))\n",
    "# runspeed = round(time.time()-start,3)\n",
    "# print(runspeed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q1rPSGsHe98f"
   },
   "source": [
    "### iii. Word Dictionary Index Sequencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 284,
     "status": "ok",
     "timestamp": 1633764807250,
     "user": {
      "displayName": "JM Lim",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12752115953509725575"
     },
     "user_tz": -480
    },
    "id": "mlq5_Xf7aYjw",
    "outputId": "82b41c59-e8db-4e73-9301-f64576f3fda2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Space Information based on 50 words index vectorization:\n",
      "Train data feature space shape      : (4085, 30)\n",
      "Validation data feature space shape : (876, 30)\n",
      "Test data feature space shape       : (876, 30)\n",
      "Size of Vocabulary                  : 6389\n"
     ]
    }
   ],
   "source": [
    "# Function for padding vectors\n",
    "def CreateIndexMatrix(data, tokenizer=tk, sms_length=50, pad_mode='post'):\n",
    "    mat = tokenizer.texts_to_sequences(data)\n",
    "    mat = pad_sequences(mat, maxlen=sms_length, padding=pad_mode)\n",
    "    return mat\n",
    "\n",
    "# Size 50 words length vectorization\n",
    "size30 = 30\n",
    "X_train_30idx = CreateIndexMatrix(X_train_sent, tk, size30)\n",
    "X_val_30idx = CreateIndexMatrix(X_val_sent, tk, size30)\n",
    "X_test_30idx = CreateIndexMatrix(X_test_sent, tk, size30)\n",
    "\n",
    "print('Feature Space Information based on 50 words index vectorization:')\n",
    "print('Train data feature space shape      : ' + str(X_train_30idx.shape))\n",
    "print('Validation data feature space shape : ' + str(X_val_30idx.shape))\n",
    "print('Test data feature space shape       : ' + str(X_test_30idx.shape))\n",
    "print('Size of Vocabulary                  : ' + str(len(list(tk.word_counts.keys()))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 472,
     "status": "ok",
     "timestamp": 1633764779009,
     "user": {
      "displayName": "JM Lim",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12752115953509725575"
     },
     "user_tz": -480
    },
    "id": "o3IrJCWL4s3r",
    "outputId": "037b5204-1631-4296-9fb5-f3479e46530f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save Complete!\n"
     ]
    }
   ],
   "source": [
    "# Saving X_30idx dataset into folder in pickle format\n",
    "X_30idx_pickle = [X_train_30idx, X_val_30idx, X_test_30idx]\n",
    "X_30idx_file = open('input_data/X_30idx.pickle', 'wb')\n",
    "pickle.dump(X_30idx_pickle, X_30idx_file)\n",
    "X_30idx_file.close()\n",
    "\n",
    "print('Save Complete!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1633764779009,
     "user": {
      "displayName": "JM Lim",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12752115953509725575"
     },
     "user_tz": -480
    },
    "id": "m93Cs4qg4_8u"
   },
   "outputs": [],
   "source": [
    "# # Testing Load Speed\n",
    "\n",
    "# start = time.time()\n",
    "# pickle_load = pickle.load(open('input_data/X_30idx.pickle', 'rb'))\n",
    "# runspeed = round(time.time()-start,3)\n",
    "# print(runspeed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1633764779009,
     "user": {
      "displayName": "JM Lim",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12752115953509725575"
     },
     "user_tz": -480
    },
    "id": "Wsjy7d7MmrRE"
   },
   "outputs": [],
   "source": [
    "# Saving Serialized Tokenizer\n",
    "tokenizer_file = open('feature_extraction_constants/bowtokenizer.pickle', 'wb')\n",
    "pickle.dump(tk, tokenizer_file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "tokenizer_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZylHk74og6sT"
   },
   "source": [
    "## Approach 3: Pre-Trained Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "executionInfo": {
     "elapsed": 9686,
     "status": "ok",
     "timestamp": 1633764788694,
     "user": {
      "displayName": "JM Lim",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12752115953509725575"
     },
     "user_tz": -480
    },
    "id": "VJrUh45fhADV"
   },
   "outputs": [],
   "source": [
    "# Load and Read Pre-Trained Word Embeddings into Dictionaries\n",
    "\n",
    "embeddings_50d = {}\n",
    "with open(\"raw_data/glove.6B.50d.txt\", 'r') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], \"float32\")\n",
    "        embeddings_50d[word] = vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1633764788698,
     "user": {
      "displayName": "JM Lim",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12752115953509725575"
     },
     "user_tz": -480
    },
    "id": "2sEfN_x1hATR",
    "outputId": "a87b26d9-f565-4a51-93d0-4210ffe43b0c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of unique words from training dataset           : 6389\n",
      "Total number of vectorized words using 50d GloVe Embeddings  : 5181\n",
      "Total number of words not vectorized and assigned with zeros : 1208\n"
     ]
    }
   ],
   "source": [
    "# Filter pre-trained word embeddings to training dataset word vocabulary\n",
    "vec_dim_50d = len(list(embeddings_50d.values())[0])\n",
    "# vec_dim_100d = len(list(embeddings_100d.values())[0])\n",
    "# vec_dim_200d = len(list(embeddings_200d.values())[0])\n",
    "\n",
    "train_vocab = list(tk.word_index.keys())\n",
    "train_vocab_vect = {}\n",
    "for word in train_vocab:\n",
    "  vect_50d  = embeddings_50d.get(word)\n",
    "  if vect_50d is None: # If the word is not found in GloVe 50d vector dictionary, assign vector of zeros to the word\n",
    "    vect_50d = np.zeros((vec_dim_50d))\n",
    "  train_vocab_vect[word] = [vect_50d]\n",
    "\n",
    "dict_vect = np.array(list(train_vocab_vect.values()), dtype=object)\n",
    "lost_50d  = sum([np.array_equal(np.zeros(vec_dim_50d), word) for word in dict_vect[:, 0]])\n",
    "\n",
    "print('Total number of unique words from training dataset           : ' + str(len(train_vocab)))\n",
    "print('Total number of vectorized words using 50d GloVe Embeddings  : ' + str(len(train_vocab) - lost_50d))\n",
    "print('Total number of words not vectorized and assigned with zeros : ' + str(lost_50d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1633764788698,
     "user": {
      "displayName": "JM Lim",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12752115953509725575"
     },
     "user_tz": -480
    },
    "id": "clEpt91OAVMr"
   },
   "outputs": [],
   "source": [
    "# Extract pre-trained embeddings based on training vocabulary words\n",
    "train_50d_dic = {}\n",
    "for word in list(train_vocab_vect.keys()):\n",
    "  if not(np.array_equal(train_vocab_vect[word][0], np.zeros(50))):\n",
    "    train_50d_dic[word] = train_vocab_vect[word][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "executionInfo": {
     "elapsed": 332,
     "status": "ok",
     "timestamp": 1633764789025,
     "user": {
      "displayName": "JM Lim",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12752115953509725575"
     },
     "user_tz": -480
    },
    "id": "5p-zjZMuDxiJ"
   },
   "outputs": [],
   "source": [
    "# Save Pre-trained GloVe Embeddings\n",
    "with open('feature_extraction_constants/embeddingsglove.pickle', 'wb') as f:\n",
    "    pickle.dump(train_50d_dic, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 291,
     "status": "ok",
     "timestamp": 1633764789314,
     "user": {
      "displayName": "JM Lim",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12752115953509725575"
     },
     "user_tz": -480
    },
    "id": "Hzbbs-FkrGrs",
    "outputId": "30ed71ce-0330-4d58-ac14-d764a6b84370"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Shapes of train dataset of word length 50 and embedding size 50 is       : (4085, 30, 50)\n",
      "Shapes of validation dataset of word length 50 and embedding size 50 is  : (876, 30, 50)\n",
      "Shapes of test dataset of word length 50 and embedding size 50 is        : (876, 30, 50)\n",
      "-------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Function for transforming data using pre-trained GloVe embeddings on local vocabulary\n",
    "def convert_to_emb(tokens, emb_dim, sms_length=50):\n",
    "    vect = np.zeros((sms_length, emb_dim))\n",
    "    for i in np.arange(min(len(tokens), sms_length)):\n",
    "      get_vect = train_vocab_vect.get(tokens[i])\n",
    "      if get_vect is not None:\n",
    "        vect[i] = get_vect[0]\n",
    "    return vect\n",
    "\n",
    "print(' ')\n",
    "# Embedding dim = 50, tokens lenght = 30\n",
    "mlen = 30\n",
    "emb = 50\n",
    "X_train_vec_30x50 = np.array([convert_to_emb(tokens, emb, mlen) for tokens in X_train_token])\n",
    "X_val_vec_30x50 = np.array([convert_to_emb(tokens, emb, mlen) for tokens in X_val_token])\n",
    "X_test_vec_30x50 = np.array([convert_to_emb(tokens, emb, mlen) for tokens in X_test_token])\n",
    "print('Shapes of train dataset of word length 50 and embedding size 50 is       : ' + str(X_train_vec_30x50.shape))\n",
    "print('Shapes of validation dataset of word length 50 and embedding size 50 is  : ' + str(X_val_vec_30x50.shape))\n",
    "print('Shapes of test dataset of word length 50 and embedding size 50 is        : ' + str(X_test_vec_30x50.shape))\n",
    "print('-------------------------------------------------------------------------------------------\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1276,
     "status": "ok",
     "timestamp": 1633764790588,
     "user": {
      "displayName": "JM Lim",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12752115953509725575"
     },
     "user_tz": -480
    },
    "id": "b3bRbZc06XIN",
    "outputId": "02efbe9a-d998-43a4-ad01-c57230a0a99b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save Complete!\n"
     ]
    }
   ],
   "source": [
    "# Saving X_vec_30x50 dataset into folder in pickle format\n",
    "X_vec_30x50_pickle = [X_train_vec_30x50, X_val_vec_30x50, X_test_vec_30x50]\n",
    "X_vec_30x50_file = open('input_data/X_vec_30x50.pickle', 'wb')\n",
    "pickle.dump(X_vec_30x50_pickle, X_vec_30x50_file)\n",
    "X_vec_30x50_file.close()\n",
    "\n",
    "print('Save Complete!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1633764790589,
     "user": {
      "displayName": "JM Lim",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12752115953509725575"
     },
     "user_tz": -480
    },
    "id": "WFsY8fSr6W8Q"
   },
   "outputs": [],
   "source": [
    "# # Testing Load Speed\n",
    "\n",
    "# start = time.time()\n",
    "# pickle_load = pickle.load(open('input_data/X_vec_30x50.pickle', 'rb'))\n",
    "# runspeed = round(time.time()-start,3)\n",
    "# print(runspeed)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOWPhh8m588qtPUY4Lr16hn",
   "collapsed_sections": [],
   "name": "Feature Selection.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
